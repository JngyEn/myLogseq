- [[Daily Log]]
	- 今天做了这些，明后天上班压力会小很多
		- ((6857a41b-a85a-4181-9cde-7f889b0f2981))
		- ((6857a42c-256e-4152-92c2-f15ec3c8d3b5))
- ---
- 申请材料确定额外准备的就是：
	- 个人陈述
	- CV
- 需要提前准备的：
	- 健康证明
	- 资产证明
- ---
- [[面试简历内容]]
  collapsed:: true
	- RAG那个工作流的写法：RAG（检索增强生成）会不会消亡呢？ - 小黄搞AI的回答 - 知乎
	  https://www.zhihu.com/question/637421964/answer/1888165550607294546
	- 到时候上班的时候注意总结一下
	- ### RAG 项目简历素材收集清单
	  
	  这份清单将帮你把项目经历从“我用LangChain搭了个RAG”升级为“我主导优化了RAG系统的XX模块，实现了YY的业务提升”。
	- #### **第一部分：项目背景与目标 (Task/Background)**
	  
	  *这部分用来回答“为什么做这个项目？”*
	- **业务场景:**
		- 这个RAG系统具体是为哪个业务服务的？（例如：智能客服、内部知识库问答、财报分析、代码辅助？）
		- 项目启动前，业务上最大的痛点是什么？（例如：客服回答问题不准确、员工找不到文档、分析师查阅资料效率低？）
	- **技术选型理由:**
		- 为什么选择RAG方案，而不是其他方案（如：微调模型、传统的关键词搜索）？
		- 项目的初始目标/核心KPI是什么？（例如：将问题回答准确率提升到80%、将资料查找时间降低50%）
	- #### **第二部分：你的角色与职责 (Your Role)**
	  
	  *这部分用来回答“你负责了什么，解决了什么问题？”*
	- 你在项目中的正式角色是什么？（例如：后端工程师、算法工程师、Java开发组长）
	- 你具体负责RAG链路中的哪些模块？（请多选）
		- 数据预处理与切分 (Data Preprocessing/Chunking)
		- 向量化与索引构建 (Embedding & Indexing)
		- 查询处理与改写 (Query Processing/Rewriting)
		- 检索/召回 (Retrieval)
		- Prompt工程 (Prompt Engineering)
		- LLM集成与响应生成 (LLM Integration & Generation)
		- 效果评估体系建设 (Evaluation)
		- 系统性能优化（延迟、吞吐量）
	- 你被分配要解决的核心技术难题是什么？（例如：“解决特定类型问题的召回率低的问题”或“降低p99响应延迟”）
	- #### **第三部分：技术实现、挑战与优化细节 (Technical Details & Innovations)**
	  
	  *这是清单的核心，也是面试官最关心的地方。你需要在这里展示你的思考和行动。*
	  
	  **1. 数据处理 & 知识库构建**
	- 知识库源文件是什么格式？(PDF, Markdown, HTML, JSON...)
	- **文本切分策略 (Chunking Strategy):**
		- 用的是简单的固定长度切分，还是更高级的策略（如：按章节、递归字符切分、使用特定分隔符）？
		- 有没有尝试过不同的切分大小和重叠（Chunk Size & Overlap）？效果如何？
		- 有没有做**语义切分**或者**多粒度切分**（比如同时把“标题-段落”和“整个文档”作为知识单元）？
		  
		  **2. 检索阶段 (Retrieval Stage)**
	- **Embedding 模型选型:**
		- 使用了哪个Embedding模型？（例如：BGE, M3E, OpenAI a-da-002, 或是公司自研模型？）
		- 为什么选这个模型？有做过不同模型的对比测试吗？
	- **Query 处理:**
		- 是否做了**查询重写 (Query Rewriting)** ？比如当用户问题很模糊时，先让LLM把问题变得更清晰、更具体。
		- 是否用了**多路查询 (Multi-Query)** ？即把一个复杂问题分解成多个子问题去分别检索。
	- **索引与召回 (Indexing & Retrieval):**
		- 使用了什么向量数据库/库？(FAISS, Milvus, Elasticsearch Vector Search, Pinecone?)
		- 召回了多少个(Top-k)文档片段？这个数字是如何决定的？
		- 除了向量检索，是否融合了其他检索方式？（如：**混合搜索 Hybrid Search**，结合了BM25等关键词搜索）
		- 是否在召回后加入了**重排 (Reranking)** 步骤来优化最终送给LLM的文档顺序？
		  
		  **3. 生成阶段 (Generation Stage)**
	- **Prompt 设计:**
		- Prompt模板是怎样设计的？有没有包含特殊的指令（如：”如果知识库内容无法回答，请直接说不知道“）来**抑制幻觉**？
		- 当检索到的上下文(Context)很长时，是如何处理的？有没有做**上下文压缩**？
		- 如何在Prompt中引导模型**引用来源 (Cite Sources)** ，提升答案的可解释性？
	- **大语言模型 (LLM):**
		- 对接了哪个LLM？（GPT-4, Claude, Llama, 通义千问, 公司自研模型？）
		- 调用LLM时，调整过哪些关键参数？(如: Temperature, Top-p)
		  
		  **4. 评估与迭代 (Evaluation & Iteration)**
	- **评估方式:**
		- 如何评估**召回**的好坏？（例如：Precision@k, Recall@k）
		- 如何评估**最终答案**的好坏？（例如：人工打分、借助RAGAS等评估框架、利用另一个LLM进行打分）
	- **闭环反馈:**
		- 系统有没有收集用户的反馈机制？（如：点赞/点踩、评分）
		- 这些反馈数据是如何用来动态优化系统的？（例如：分析差评case，反过来优化Prompt或数据）
	- #### **第四部分：量化成果与业务价值 (Quantifiable Results)**
	  
	  *这是你简历中最亮眼的部分。请务必找到具体的数字来支撑你的贡献！*
	  
	  **“之前” vs "之后" 的对比数据：**
	- **准确性 / 相关性指标:**
		- **召回准确率 (Retrieval Precision/Recall):** Top-k召回准确率从 `__%` 提升到 `__%`？
		- **答案准确率 (Answer Relevancy):** 评估的平均分（如1-5分）从 `__` 提升到 `__`？
		- **幻觉比例 (Hallucination Rate):** 模型胡说八道的比例降低了 `__%`？
		- **无有效答案率 (No Answer Rate):** 对于知识库外问题，系统正确回答“不知道”的比例从 `__%` 提升到 `__%`？
	- **效率 / 性能指标:**
		- **响应时间 (Latency):** 平均响应时间从 `__` 秒降低到 `__` 秒？
		- **系统吞吐量 (Throughput):** 系统每秒能处理的查询数(QPS)提升了多少？
		- **成本节约 (Cost Reduction):** 通过Prompt优化或模型选择，平均每次查询的Token消耗/费用降低了 `__%`？
	- **业务 / 用户指标:**
		- **用户满意度:** 用户满意度问卷得分从 `__` 提高到 `__`？
		- **业务指标:** （如客服场景）问题解决率提升 `__%`？人工客服介入率降低 `__%`？